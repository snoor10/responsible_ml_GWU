{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln5JM1en77Dx"
      },
      "source": [
        "####License\n",
        "Copyright 2023 Priyanka Bhatia (priyanka.bhatia@gwu.edu), Mia Lakstigala (mia.lakstigala@gwmail.gwu.edu),Brenden Moore (brendenmoore@gwmail.gwu.edu), Sadman Noor (snoor11@gwmail.gwu.edu),\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "DISCLAIMER: This notebook is not legal or compliance advice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_b7K8Ib8LLY"
      },
      "source": [
        "# Assignment 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njEtt4xw8QS_"
      },
      "source": [
        "### Imports and inits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8faCo2t84me"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXMoc2CrNh8E"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMXgubiXAhbd",
        "outputId": "f91299b2-5732-44fc-da9d-082928f186cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h2o\n",
            "  Downloading h2o-3.40.0.4.tar.gz (177.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.8.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from h2o) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.40.0.4-py2.py3-none-any.whl size=177697886 sha256=0eb052ed68e90920dae8812481d1e2488d3049280056582b2c13e82036483653\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f2/b0/5bb4d702a0467e82d77c45088db3eef25114c26b0eec8e7f6a\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.40.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apGk3SaeAmIo",
        "outputId": "ab1ad0a8-0e06-4f7c-a3ba-548e5b01d2c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting interpret\n",
            "  Downloading interpret-0.4.2-py3-none-any.whl (1.4 kB)\n",
            "Collecting interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2 (from interpret)\n",
            "  Downloading interpret_core-0.4.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lime>=0.1.1.33 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (5.5.6)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (7.34.0)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (5.9.5)\n",
            "Requirement already satisfied: numpy<1.24.0,>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.10.1)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.2.2)\n",
            "Collecting SALib>=1.3.3 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading salib-1.4.7-py3-none-any.whl (757 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m758.0/758.0 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (5.13.1)\n",
            "Collecting shap>=0.28.5 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.2.5 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting skope-rules>=1.0.1 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading skope_rules-1.0.1-py3-none-any.whl (14 kB)\n",
            "Collecting dash>=1.0.0 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading dash-2.10.2-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dash-core-components>=1.0.0 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-html-components>=1.0.0 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-table>=4.1.0 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting dash-cytoscape>=0.1.1 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading dash_cytoscape-0.3.0-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gevent>=1.3.6 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading gevent-22.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.27.1)\n",
            "Collecting treeinterpreter>=0.2.2 (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading treeinterpreter-0.2.3-py2.py3-none-any.whl (6.0 kB)\n",
            "Requirement already satisfied: Flask<2.3.0,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.2.4)\n",
            "Collecting Werkzeug<2.3.0 (from dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.event (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (67.7.2)\n",
            "Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.0.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (6.3.1)\n",
            "Collecting jedi>=0.16 (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (4.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (4.65.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2022.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=3.8.1->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (8.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.4)\n",
            "Collecting multiprocess (from SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.1->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.1.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (23.1)\n",
            "Collecting slicer==0.0.7 (from shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.2.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (8.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.8.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<2.3.0->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (2.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.10.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (5.3.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.10.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (23.2.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (0.39.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel>=4.10.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]==0.4.2->interpret) (3.3.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283839 sha256=8e4fdc7cafb637063b2c36d2ceed2043d5a209b483ae4b45a0f029f5cdf7c90c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: treeinterpreter, interpret-core, dash-table, dash-html-components, dash-core-components, zope.interface, zope.event, Werkzeug, slicer, jedi, dill, multiprocess, gevent, skope-rules, shap, SALib, lime, dash, dash-cytoscape, interpret\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 2.3.0\n",
            "    Uninstalling Werkzeug-2.3.0:\n",
            "      Successfully uninstalled Werkzeug-2.3.0\n",
            "Successfully installed SALib-1.4.7 Werkzeug-2.2.3 dash-2.10.2 dash-core-components-2.0.0 dash-cytoscape-0.3.0 dash-html-components-2.0.0 dash-table-5.0.0 dill-0.3.6 gevent-22.10.2 interpret-0.4.2 interpret-core-0.4.2 jedi-0.18.2 lime-0.2.0.1 multiprocess-0.70.14 shap-0.41.0 skope-rules-1.0.1 slicer-0.0.7 treeinterpreter-0.2.3 zope.event-4.6 zope.interface-6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install interpret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vr5EhkFAxqW"
      },
      "outputs": [],
      "source": [
        "from interpret.glassbox import ExplainableBoostingClassifier  # interpret ebm class\n",
        "from interpret.perf import ROC                                # ROC measure for ebm\n",
        "import itertools                                              # for cartesian product of parameters\n",
        "import matplotlib.pyplot as plt                               # for plots\n",
        "import numpy as np                                            # for basic array manipulation\n",
        "import pandas as pd                                           # for dataframe manipulation\n",
        "import random                                                 # to sample from lists\n",
        "from sklearn.metrics import accuracy_score, f1_score          # for selecting model cutoffs\n",
        "import time                                                   # for timers\n",
        "\n",
        "# set numpy random seed for better reproducibility\n",
        "SEED = 12345\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# set number of threads\n",
        "NTHREAD = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMYwfaGCA2TC"
      },
      "source": [
        "#### Define Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkdvWmHXA6UZ"
      },
      "source": [
        "##### Utility function to calculate confusion matrices by demographic group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhyeqCjrA84a"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(frame, y, yhat, by=None, level=None, cutoff=0.5, verbose=True):\n",
        "\n",
        "    \"\"\" Creates confusion matrix from pandas dataframe of y and yhat values, can be sliced\n",
        "        by a variable and level.\n",
        "\n",
        "        :param frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
        "        :param y: Name of actual value column.\n",
        "        :param yhat: Name of predicted value column.\n",
        "        :param by: By variable to slice frame before creating confusion matrix, default None.\n",
        "        :param level: Value of by variable to slice frame before creating confusion matrix, default None.\n",
        "        :param cutoff: Cutoff threshold for confusion matrix, default 0.5.\n",
        "        :param verbose: Whether to print confusion matrix titles, default True.\n",
        "        :return: Confusion matrix as pandas dataframe.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # determine levels of target (y) variable\n",
        "    # sort for consistency\n",
        "    level_list = list(frame[y].unique())\n",
        "    level_list.sort(reverse=True)\n",
        "\n",
        "    # init confusion matrix\n",
        "    cm_frame = pd.DataFrame(columns=['actual: ' +  str(i) for i in level_list],\n",
        "                            index=['predicted: ' + str(i) for i in level_list])\n",
        "\n",
        "    # don't destroy original data\n",
        "    frame_ = frame.copy(deep=True)\n",
        "\n",
        "    # convert numeric predictions to binary decisions using cutoff\n",
        "    dname = 'd_' + str(y)\n",
        "    frame_[dname] = np.where(frame_[yhat] > cutoff , 1, 0)\n",
        "\n",
        "    # slice frame\n",
        "    if (by is not None) & (level is not None):\n",
        "        frame_ = frame_[frame[by] == level]\n",
        "\n",
        "    # calculate size of each confusion matrix value\n",
        "    for i, lev_i in enumerate(level_list):\n",
        "        for j, lev_j in enumerate(level_list):\n",
        "            cm_frame.iat[j, i] = frame_[(frame_[y] == lev_i) & (frame_[dname] == lev_j)].shape[0]\n",
        "            # i, j vs. j, i nasty little bug ... updated 8/30/19\n",
        "\n",
        "    # output results\n",
        "    if verbose:\n",
        "        if by is None:\n",
        "            print('Confusion matrix:')\n",
        "        else:\n",
        "            print('Confusion matrix by ' + by + '=' + str(level))\n",
        "\n",
        "    return cm_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-K12mhFBB8L"
      },
      "source": [
        "#### Utility function to calculate AIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp5wDyqEBDOX"
      },
      "outputs": [],
      "source": [
        "def air(cm_dict, reference_key, protected_key, verbose=True):\n",
        "\n",
        "    \"\"\" Calculates the adverse impact ratio as a quotient between protected and\n",
        "        reference group acceptance rates: protected_prop/reference_prop.\n",
        "        Optionally prints intermediate values. ASSUMES 0 IS \"POSITIVE\" OUTCOME!\n",
        "\n",
        "        :param cm_dict: Dictionary of demographic group confusion matrices.\n",
        "        :param reference_key: Name of reference group in cm_dict as a string.\n",
        "        :param protected_key: Name of protected group in cm_dict as a string.\n",
        "        :param verbose: Whether to print intermediate acceptance rates, default True.\n",
        "        :return: AIR.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    eps = 1e-20 # numeric stability and divide by 0 protection\n",
        "\n",
        "    # reference group summary\n",
        "    reference_accepted = float(cm_dict[reference_key].iat[1,0] + cm_dict[reference_key].iat[1,1]) # predicted 0's\n",
        "    reference_total = float(cm_dict[reference_key].sum().sum())\n",
        "    reference_prop = reference_accepted/reference_total\n",
        "    if verbose:\n",
        "        print(reference_key.title() + ' proportion accepted: %.3f' % reference_prop)\n",
        "\n",
        "    # protected group summary\n",
        "    protected_accepted = float(cm_dict[protected_key].iat[1,0] + cm_dict[protected_key].iat[1,1]) # predicted 0's\n",
        "    protected_total = float(cm_dict[protected_key].sum().sum())\n",
        "    protected_prop = protected_accepted/protected_total\n",
        "    if verbose:\n",
        "        print(protected_key.title() + ' proportion accepted: %.3f' % protected_prop)\n",
        "\n",
        "    # return adverse impact ratio\n",
        "    return ((protected_prop + eps)/(reference_prop + eps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvsPcUNuBHFa"
      },
      "source": [
        "#### Utility function to select probability cutoff by F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGb2JsyNBJQt"
      },
      "outputs": [],
      "source": [
        "def get_max_f1_frame(frame, y, yhat, res=0.01, air_reference=None, air_protected=None):\n",
        "\n",
        "    \"\"\" Utility function for finding max. F1.\n",
        "        Coupled to get_confusion_matrix() and air().\n",
        "        Assumes 1 is the marker for class membership.\n",
        "\n",
        "        :param frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
        "        :param y: Known y values.\n",
        "        :param yhat: Model scores.\n",
        "        :param res: Resolution over which to search for max. F1, default 0.01.\n",
        "        :param air_reference: Reference group for AIR calculation, optional.\n",
        "        :param air_protected: Protected group for AIR calculation, optional.\n",
        "        :return: Pandas DataFrame of cutoffs to select from.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    do_air = all(v is not None for v in [air_reference, air_protected])\n",
        "\n",
        "    # init frame to store f1 at different cutoffs\n",
        "    if do_air:\n",
        "        columns = ['cut', 'f1', 'acc', 'air']\n",
        "    else:\n",
        "        columns = ['cut', 'f1', 'acc']\n",
        "    f1_frame = pd.DataFrame(columns=['cut', 'f1', 'acc'])\n",
        "\n",
        "    # copy known y and score values into a temporary frame\n",
        "    temp_df = frame[[y, yhat]].copy(deep=True)\n",
        "\n",
        "    # find f1 at different cutoffs and store in acc_frame\n",
        "    for cut in np.arange(0, 1 + res, res):\n",
        "        temp_df['decision'] = np.where(temp_df.iloc[:, 1] > cut, 1, 0)\n",
        "        f1 = f1_score(temp_df.iloc[:, 0], temp_df['decision'])\n",
        "        acc = accuracy_score(temp_df.iloc[:, 0], temp_df['decision'])\n",
        "        row_dict = {'cut': cut, 'f1': f1, 'acc': acc}\n",
        "        if do_air:\n",
        "            # conditionally calculate AIR\n",
        "            cm_ref = get_confusion_matrix(frame, y, yhat, by=air_reference, level=1, cutoff=cut, verbose=False)\n",
        "            cm_pro = get_confusion_matrix(frame, y, yhat, by=air_protected, level=1, cutoff=cut, verbose=False)\n",
        "            air_ = air({air_reference: cm_ref, air_protected: cm_pro}, air_reference, air_protected, verbose=False)\n",
        "            row_dict['air'] = air_\n",
        "\n",
        "        f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
        "\n",
        "    del temp_df\n",
        "\n",
        "    return f1_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ndZPH92BNSC"
      },
      "source": [
        "#### Utility function for random grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MOpGgILBShz"
      },
      "outputs": [],
      "source": [
        "def ebm_grid(train, valid, x_names, y_name, gs_params=None, n_models=None, early_stopping_rounds=None, seed=None,\n",
        "             air_reference=None, air_protected=None, air_cut=None):\n",
        "\n",
        "    \"\"\" Performs a random grid search over n_models and gs_params.\n",
        "        Optionally considers random feature sets and AIR.\n",
        "        Coupled to get_confusion_matrix() and air().\n",
        "\n",
        "    :param train: Training data as Pandas DataFrame.\n",
        "    :param valid: Validation data as Pandas DataFrame.\n",
        "    :param x_names: Names of input features.\n",
        "    :param y_name: Name of target feature.\n",
        "    :param gs_params: Dictionary of lists of potential EBM parameters over which to search.\n",
        "    :param n_models: Number of random models to evaluate.\n",
        "    :param early_stopping_rounds: EBM early stopping rounds.\n",
        "    :param seed: Random seed for better interpretability.\n",
        "    :param air_reference: Reference group for AIR calculation, optional.\n",
        "    :param air_protected: Protected group for AIR calculation, optional.\n",
        "    :param air_cut: Cutoff for AIR calculation, optional.\n",
        "    :return: Tuple: (Best EBM model, Pandas DataFrame of models to select from)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # init returned frame\n",
        "    do_air = all(v is not None for v in [air_reference, air_protected])\n",
        "    if do_air:\n",
        "        columns = list(gs_params.keys()) + ['features', 'auc', 'air']\n",
        "    else:\n",
        "        columns = list(gs_params.keys()) + ['auc']\n",
        "    ebm_grid_frame = pd.DataFrame(columns=columns)\n",
        "\n",
        "    # cartesian product of gs_params\n",
        "    keys, values = zip(*gs_params.items())\n",
        "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    # preserve exact reproducibility for this function\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    # select randomly from cartesian product space\n",
        "    selected_experiments = np.random.choice(len(experiments), n_models)\n",
        "\n",
        "    # set global params for seed, etc.\n",
        "    params = {'n_jobs': NTHREAD,\n",
        "              'early_stopping_rounds': early_stopping_rounds,\n",
        "              'random_state': SEED}\n",
        "\n",
        "    # init grid search loop\n",
        "    best_candidate = None\n",
        "    best_score = 0\n",
        "\n",
        "    # grid search loop\n",
        "    for i, exp in enumerate(selected_experiments):\n",
        "\n",
        "        params.update(experiments[exp])  # override global params with current grid run params\n",
        "\n",
        "        print('Grid search run %d/%d:' % (int(i + 1), int(n_models)))\n",
        "        print('Training with parameters:', params)\n",
        "\n",
        "        # train\n",
        "        ebm = ExplainableBoostingClassifier(**params)\n",
        "\n",
        "        # conditionally select random features\n",
        "        features = x_names\n",
        "        if do_air:\n",
        "            n_features = random.randrange(len(x_names)) + 1\n",
        "            features = random.sample(x_names, n_features)\n",
        "        candidate = ebm.fit(train[features], train[y_name])\n",
        "\n",
        "        # calculate AUC\n",
        "        ebm_perf = ROC(ebm.predict_proba).explain_perf(valid[features], valid[y_name])\n",
        "        candidate_best_score = ebm_perf._internal_obj['overall']['auc']\n",
        "\n",
        "        # compose values to add to ebm_grid_frame\n",
        "        row_dict = params.copy()\n",
        "        row_dict['auc'] = candidate_best_score\n",
        "        if do_air:\n",
        "            # collect random feature set\n",
        "            row_dict['features'] = features\n",
        "            # conditionally calculate AIR\n",
        "            valid_phat = valid.copy(deep=True)\n",
        "            valid_phat['phat'] = candidate.predict_proba(valid[features])[:, 1]\n",
        "            cm_ref = get_confusion_matrix(valid_phat, y_name, 'phat', by=air_reference, level=1, cutoff=air_cut, verbose=False)\n",
        "            cm_pro = get_confusion_matrix(valid_phat, y_name, 'phat', by=air_protected, level=1, cutoff=air_cut, verbose=False)\n",
        "            air_ = air({air_reference: cm_ref, air_protected: cm_pro}, air_reference, air_protected, verbose=False)\n",
        "            row_dict['air'] = air_\n",
        "            del valid_phat\n",
        "\n",
        "        # append run to ebm_grid_frame\n",
        "        ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n",
        "\n",
        "        # determine if current model is better than previous best\n",
        "        if candidate_best_score > best_score:\n",
        "            best_score = candidate_best_score\n",
        "            best_ebm = candidate\n",
        "            print('Grid search new best score discovered at iteration %d/%d: %.4f.' %\n",
        "                             (int(i + 1), int(n_models), candidate_best_score))\n",
        "\n",
        "        print('---------- ----------')\n",
        "\n",
        "        del row_dict\n",
        "        del ebm\n",
        "\n",
        "    return best_ebm, ebm_grid_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAdioYwsBVTj"
      },
      "source": [
        "#### Start global timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8tawgbZBYo-"
      },
      "outputs": [],
      "source": [
        "tic = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvLx4OFUBafX"
      },
      "source": [
        "#### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "-U-unYIJBcwz",
        "outputId": "8f60d0ca-e4be-4b10-ccf1-0e2da4db9386"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-591d5b9c-5d11-4559-b103-980407da4467\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-591d5b9c-5d11-4559-b103-980407da4467\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving hmda_test_preprocessed.csv to hmda_test_preprocessed.csv\n",
            "Saving hmda_train_preprocessed.csv to hmda_train_preprocessed.csv\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7lt4nyoCOTu"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('hmda_train_preprocessed.csv')\n",
        "test = pd.read_csv('hmda_test_preprocessed.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5JY54DaCRcd"
      },
      "source": [
        "#### Assign basic modeling roles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUi9a2EQCT9x"
      },
      "outputs": [],
      "source": [
        "y_name = 'high_priced'\n",
        "x_names = ['term_360', 'conforming', 'debt_to_income_ratio_missing', 'loan_amount_std', 'loan_to_value_ratio_std', 'no_intro_rate_period_std',\n",
        "           'intro_rate_period_std', 'property_value_std', 'income_std', 'debt_to_income_ratio_std']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dyVyjGMCV7J"
      },
      "source": [
        "### Fit interpretable model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL41cleoCYQT"
      },
      "source": [
        "#### Split data into train and validation partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5w35fYiCbke",
        "outputId": "d861cfcc-d8aa-48a9-db6d-0ed01cb7b6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data rows = 112253, columns = 23\n",
            "Validation data rows = 48085, columns = 23\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(SEED) # preserve exact reproducibility for this cell\n",
        "\n",
        "split_ratio = 0.7 # 70%/30% train/test split\n",
        "\n",
        "# execute split\n",
        "split = np.random.rand(len(data)) < split_ratio\n",
        "train = data[split]\n",
        "valid = data[~split]\n",
        "\n",
        "# summarize split\n",
        "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
        "print('Validation data rows = %d, columns = %d' % (valid.shape[0], valid.shape[1]))\n",
        "\n",
        "# benchmark - Train data rows = 112253, columns = 23\n",
        "# benchmark - Validation data rows = 48085, columns = 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl0Kvl6VCdB2"
      },
      "source": [
        "### Explainable Boosting Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW-LYYonCj_S"
      },
      "source": [
        "### Fit EBM with random grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__SeqlXRCpQB",
        "outputId": "74b789c1-a4b3-46b3-fed6-9d07d11ea1ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid search run 1/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid search new best score discovered at iteration 1/10: 0.8218.\n",
            "---------- ----------\n",
            "Grid search run 2/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- ----------\n",
            "Grid search run 3/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- ----------\n",
            "Grid search run 4/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- ----------\n",
            "Grid search run 5/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid search new best score discovered at iteration 5/10: 0.8253.\n",
            "---------- ----------\n",
            "Grid search run 6/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- ----------\n",
            "Grid search run 7/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- ----------\n",
            "Grid search run 8/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 3}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- ----------\n",
            "Grid search run 9/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- ----------\n",
            "Grid search run 10/10:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "EBM training completed in 1942.05 s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-1bf56b7a0729>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "# dictionary of hyperparameter value lists for grid search\n",
        "gs_params = {'max_bins': [128, 256, 512],\n",
        "             'max_interaction_bins': [16, 32, 64],\n",
        "             'interactions': [5, 10, 15],\n",
        "             'outer_bags': [4, 8, 12],\n",
        "             'inner_bags': [0, 4],\n",
        "             'learning_rate': [0.001, 0.01, 0.05],\n",
        "             'validation_size': [0.1, 0.25, 0.5],\n",
        "             'min_samples_leaf': [1, 2, 5, 10],\n",
        "             'max_leaves': [1, 3, 5]}\n",
        "\n",
        "# start local timer\n",
        "ebm_tic = time.time()\n",
        "\n",
        "# EBM grid search\n",
        "best_ebm, ebm_grid_frame = ebm_grid(train, valid, x_names, y_name, gs_params=gs_params, n_models=10,\n",
        "                                    early_stopping_rounds=100, seed=SEED)\n",
        "\n",
        "# end local timer\n",
        "ebm_toc = time.time() - ebm_tic\n",
        "print('EBM training completed in %.2f s.' % (ebm_toc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vpEgHf3KNq0"
      },
      "source": [
        "### Basic AUC assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfcsm0hcKRNO",
        "outputId": "113e412c-26c1-41fc-faa6-502181544d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation AUC: 0.8253.\n"
          ]
        }
      ],
      "source": [
        "best_ebm_perf = ROC(best_ebm.predict_proba).explain_perf(valid[x_names], valid[y_name])\n",
        "print('Validation AUC: %.4f.' % best_ebm_perf._internal_obj['overall']['auc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za1FIz7DKVuq"
      },
      "source": [
        "### Score validation data with model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "3k9Nu4NUKa4l",
        "outputId": "03de93c3-bad2-483f-e43e-3d018b2a2a86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-823d72e4-2841-4f11-a501-e1e4bd482382\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>black</th>\n",
              "      <th>asian</th>\n",
              "      <th>white</th>\n",
              "      <th>amind</th>\n",
              "      <th>hipac</th>\n",
              "      <th>hispanic</th>\n",
              "      <th>non_hispanic</th>\n",
              "      <th>male</th>\n",
              "      <th>female</th>\n",
              "      <th>...</th>\n",
              "      <th>debt_to_income_ratio_missing</th>\n",
              "      <th>loan_amount_std</th>\n",
              "      <th>loan_to_value_ratio_std</th>\n",
              "      <th>no_intro_rate_period_std</th>\n",
              "      <th>intro_rate_period_std</th>\n",
              "      <th>property_value_std</th>\n",
              "      <th>income_std</th>\n",
              "      <th>debt_to_income_ratio_std</th>\n",
              "      <th>high_priced</th>\n",
              "      <th>phat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.514393</td>\n",
              "      <td>0.333922</td>\n",
              "      <td>0.244394</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>-0.535932</td>\n",
              "      <td>-0.040307</td>\n",
              "      <td>0.854601</td>\n",
              "      <td>0</td>\n",
              "      <td>0.165646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.426448</td>\n",
              "      <td>0.355249</td>\n",
              "      <td>0.244394</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>-0.474263</td>\n",
              "      <td>-0.020904</td>\n",
              "      <td>1.037419</td>\n",
              "      <td>0</td>\n",
              "      <td>0.314594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.277109</td>\n",
              "      <td>0.142995</td>\n",
              "      <td>0.244394</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>0.111598</td>\n",
              "      <td>-0.019865</td>\n",
              "      <td>0.031916</td>\n",
              "      <td>0</td>\n",
              "      <td>0.022284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.382476</td>\n",
              "      <td>-0.240432</td>\n",
              "      <td>0.244394</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>-0.320089</td>\n",
              "      <td>-0.028181</td>\n",
              "      <td>0.946010</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.101220</td>\n",
              "      <td>-0.266529</td>\n",
              "      <td>0.244394</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>0.111598</td>\n",
              "      <td>0.016515</td>\n",
              "      <td>-1.156406</td>\n",
              "      <td>0</td>\n",
              "      <td>0.004888</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-823d72e4-2841-4f11-a501-e1e4bd482382')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-823d72e4-2841-4f11-a501-e1e4bd482382 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-823d72e4-2841-4f11-a501-e1e4bd482382');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   row_id  black  asian  white  amind  hipac  hispanic  non_hispanic  male  \\\n",
              "0       0    NaN    NaN    NaN    NaN    NaN       NaN           NaN   1.0   \n",
              "1       6    0.0    0.0    1.0    0.0    0.0       0.0           1.0   0.0   \n",
              "2       8    0.0    0.0    1.0    0.0    0.0       0.0           1.0   NaN   \n",
              "3      10    0.0    0.0    1.0    0.0    0.0       0.0           1.0   NaN   \n",
              "4      11    0.0    0.0    1.0    0.0    0.0       1.0           0.0   NaN   \n",
              "\n",
              "   female  ...  debt_to_income_ratio_missing  loan_amount_std  \\\n",
              "0     0.0  ...                             0        -0.514393   \n",
              "1     1.0  ...                             0        -0.426448   \n",
              "2     NaN  ...                             0         0.277109   \n",
              "3     NaN  ...                             0        -0.382476   \n",
              "4     NaN  ...                             0         0.101220   \n",
              "\n",
              "   loan_to_value_ratio_std  no_intro_rate_period_std  intro_rate_period_std  \\\n",
              "0                 0.333922                  0.244394              -0.215304   \n",
              "1                 0.355249                  0.244394              -0.215304   \n",
              "2                 0.142995                  0.244394              -0.215304   \n",
              "3                -0.240432                  0.244394              -0.215304   \n",
              "4                -0.266529                  0.244394              -0.215304   \n",
              "\n",
              "   property_value_std  income_std  debt_to_income_ratio_std  high_priced  \\\n",
              "0           -0.535932   -0.040307                  0.854601            0   \n",
              "1           -0.474263   -0.020904                  1.037419            0   \n",
              "2            0.111598   -0.019865                  0.031916            0   \n",
              "3           -0.320089   -0.028181                  0.946010            0   \n",
              "4            0.111598    0.016515                 -1.156406            0   \n",
              "\n",
              "       phat  \n",
              "0  0.165646  \n",
              "1  0.314594  \n",
              "2  0.022284  \n",
              "3  0.015600  \n",
              "4  0.004888  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_ebm_phat = pd.DataFrame(best_ebm.predict_proba(valid[x_names])[:, 1], columns=['phat'])\n",
        "best_ebm_phat = pd.concat([valid.reset_index(drop=True), best_ebm_phat], axis=1)\n",
        "best_ebm_phat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfhBmn27KfiO"
      },
      "source": [
        "### Investigate Best Model (EBM) for Discrimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbQUpF3MKixU"
      },
      "source": [
        "### Find optimal cutoff based on F1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enrtg4HOKmhC"
      },
      "source": [
        "##### Cutoffs are normally selected by maximizing a quality statistic or a business metric, and not by considering bias and discrimination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A34vfdq9KpX6",
        "outputId": "94e8ad14-49c4-4f3e-de0d-6cd3c1612e6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      cut        f1       acc\n",
            "0    0.00  0.173860  0.095206\n",
            "1    0.01  0.233938  0.384777\n",
            "2    0.02  0.262541  0.479048\n",
            "3    0.03  0.280733  0.530685\n",
            "4    0.04  0.295953  0.569783\n",
            "..    ...       ...       ...\n",
            "96   0.96  0.000000  0.904794\n",
            "97   0.97  0.000000  0.904794\n",
            "98   0.98  0.000000  0.904794\n",
            "99   0.99  0.000000  0.904794\n",
            "100  1.00  0.000000  0.904794\n",
            "\n",
            "[101 rows x 3 columns]\n",
            "\n",
            "Best EBM F1: 0.3666 achieved at cutoff: 0.18 with accuracy: 0.7927.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
            "<ipython-input-23-4847137f7901>:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  f1_frame = f1_frame.append(row_dict, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "f1_frame = get_max_f1_frame(best_ebm_phat, y_name, 'phat')\n",
        "\n",
        "print(f1_frame)\n",
        "print()\n",
        "\n",
        "max_f1 = f1_frame['f1'].max()\n",
        "best_cut = f1_frame.loc[int(f1_frame['f1'].idxmax()), 'cut'] #idxmax() returns the index of the maximum value\n",
        "acc = f1_frame.loc[int(f1_frame['f1'].idxmax()), 'acc']\n",
        "\n",
        "print('Best EBM F1: %.4f achieved at cutoff: %.2f with accuracy: %.4f.' % (max_f1, best_cut, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3CzpdXmKtxT"
      },
      "source": [
        "### Find confusion matrices for demographic groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSTTmZP3K1a5",
        "outputId": "5025c346-3e03-460f-8384-986531b72e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix by black=1\n",
            "             actual: 1 actual: 0\n",
            "predicted: 1       470       911\n",
            "predicted: 0       194      1617\n",
            "\n",
            "Confusion matrix by asian=1\n",
            "             actual: 1 actual: 0\n",
            "predicted: 1        95       176\n",
            "predicted: 0        53      2926\n",
            "\n",
            "Confusion matrix by white=1\n",
            "             actual: 1 actual: 0\n",
            "predicted: 1      1965      6117\n",
            "predicted: 0      1200     25243\n",
            "\n",
            "Confusion matrix by male=1\n",
            "             actual: 1 actual: 0\n",
            "predicted: 1      1036      3122\n",
            "predicted: 0       628     11046\n",
            "\n",
            "Confusion matrix by female=1\n",
            "             actual: 1 actual: 0\n",
            "predicted: 1       847      2175\n",
            "predicted: 0       393      6617\n",
            "\n"
          ]
        }
      ],
      "source": [
        "demographic_group_names = ['black', 'asian', 'white', 'male', 'female']\n",
        "cm_dict = {}\n",
        "\n",
        "for name in demographic_group_names:\n",
        "    cm_dict[name] = get_confusion_matrix(best_ebm_phat, y_name, 'phat', by=name, level=1, cutoff=best_cut)\n",
        "    print(cm_dict[name])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckPABH3VK5Cp"
      },
      "source": [
        "### Find AIR for Asian people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFI0AxPOK7im",
        "outputId": "a0f892b0-b107-4ce4-9543-f086b373d210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "White proportion accepted: 0.766\n",
            "Asian proportion accepted: 0.917\n",
            "Adverse impact ratio for Asian people vs. White people: 1.197\n"
          ]
        }
      ],
      "source": [
        "print('Adverse impact ratio for Asian people vs. White people: %.3f' % air(cm_dict, 'white', 'asian'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP02ZEsGM-EW"
      },
      "source": [
        "### Find AIR for Black people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR9VfRjPNC8h",
        "outputId": "18e78295-4aed-4035-a317-9a8b9c7a20d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "White proportion accepted: 0.766\n",
            "Black proportion accepted: 0.567\n",
            "Adverse impact ratio for Black people vs. White people: 0.741\n"
          ]
        }
      ],
      "source": [
        "print('Adverse impact ratio for Black people vs. White people: %.3f' % air(cm_dict, 'white', 'black'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZTg-jDoNGpK"
      },
      "source": [
        "### Find AIR for Females"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKNv9I_yNJoC",
        "outputId": "9c3979e7-3f5e-49d5-a19d-3bf84aafa082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Male proportion accepted: 0.737\n",
            "Female proportion accepted: 0.699\n",
            "Adverse impact ratio for Females vs. Males: 0.948\n"
          ]
        }
      ],
      "source": [
        "print('Adverse impact ratio for Females vs. Males: %.3f' % air(cm_dict, 'male', 'female'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsqnOUghNNAG"
      },
      "source": [
        "## Attempt remediation of discovered discrimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWSle2pyNQSX"
      },
      "source": [
        "### Simplest remediation: Find cutoff with better Black vs. White AIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bU2Y0IQlNTJ1",
        "outputId": "8c8fee50-25c7-41a0-fb37-64e3191067e0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3d72097a-2fce-45b1-879a-1ddaa1f5969a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cut</th>\n",
              "      <th>f1</th>\n",
              "      <th>acc</th>\n",
              "      <th>air</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.356794</td>\n",
              "      <td>0.832942</td>\n",
              "      <td>0.816260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.350179</td>\n",
              "      <td>0.841156</td>\n",
              "      <td>0.843019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.341709</td>\n",
              "      <td>0.850161</td>\n",
              "      <td>0.864101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.330154</td>\n",
              "      <td>0.858313</td>\n",
              "      <td>0.878680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.26</td>\n",
              "      <td>0.316466</td>\n",
              "      <td>0.865155</td>\n",
              "      <td>0.887407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d72097a-2fce-45b1-879a-1ddaa1f5969a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d72097a-2fce-45b1-879a-1ddaa1f5969a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d72097a-2fce-45b1-879a-1ddaa1f5969a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     cut        f1       acc       air\n",
              "22  0.22  0.356794  0.832942  0.816260\n",
              "23  0.23  0.350179  0.841156  0.843019\n",
              "24  0.24  0.341709  0.850161  0.864101\n",
              "25  0.25  0.330154  0.858313  0.878680\n",
              "26  0.26  0.316466  0.865155  0.887407"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_frame = get_max_f1_frame(best_ebm_phat, y_name, 'phat', air_reference='white', air_protected='black')\n",
        "# print highest quality cutoffs above four fifths rule cutoff\n",
        "f1_frame[f1_frame['air'] > 0.8].sort_values(by='f1', ascending=False).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBNoES8LNpbo"
      },
      "source": [
        "##### Cutoffs in the 0.22-0.26 range provide increased accuracy and less bias towards Black people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMFmyWeeNuIb"
      },
      "source": [
        "### Check that other groups are not adversely impacted by change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jPSXAauNtM9",
        "outputId": "f6c66ee8-0979-483b-89e7-0175b151877b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adverse impact ratio for Asian people vs. White people: 1.128\n",
            "Adverse impact ratio for Black people vs. White people: 0.816\n",
            "Adverse impact ratio for Females vs. Males: 0.965\n"
          ]
        }
      ],
      "source": [
        "# calculate new confusion matrics for each group\n",
        "rem_cm_dict = {}\n",
        "for name in demographic_group_names:\n",
        "    rem_cm_dict[name] = get_confusion_matrix(best_ebm_phat, y_name, 'phat', by=name, level=1, cutoff=0.22, verbose=False)\n",
        "\n",
        "# calculate AIR for each group\n",
        "print('Adverse impact ratio for Asian people vs. White people: %.3f' % air(rem_cm_dict, 'white', 'asian', verbose=False))\n",
        "print('Adverse impact ratio for Black people vs. White people: %.3f' % air(rem_cm_dict, 'white', 'black', verbose=False))\n",
        "print('Adverse impact ratio for Females vs. Males: %.3f' % air(rem_cm_dict, 'male', 'female', verbose=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKx0dDUjN0AR"
      },
      "source": [
        "##### The new cutoff does not adversely affect other protected groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJfmto3jN586"
      },
      "source": [
        "#### More sophisticated remediation: Model selection via quality and fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG8eBkUeN3gw",
        "outputId": "a7a4b3ef-3909-4180-d4ca-3bdc8a171a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid search run 1/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "Grid search new best score discovered at iteration 1/500: 0.8204.\n",
            "---------- ----------\n",
            "Grid search run 2/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 3/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 4/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 5/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 6/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 7/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 8/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 9/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 10/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 11/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 12/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 13/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 14/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 15/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 16/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 17/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 18/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 19/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "Grid search new best score discovered at iteration 19/500: 0.8229.\n",
            "---------- ----------\n",
            "Grid search run 20/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 21/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 22/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 23/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 24/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 25/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 26/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 27/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 28/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 29/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 30/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 31/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 32/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 33/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 34/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 35/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 36/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 37/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 38/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 39/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 40/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 41/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 42/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 43/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 44/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 45/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 46/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 47/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 48/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 49/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 50/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 51/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 52/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 53/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 54/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 55/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 56/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 57/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 58/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 59/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 60/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 61/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 62/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 63/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 64/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 65/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 66/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 67/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 68/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 69/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 70/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 71/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 72/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 73/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 74/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 75/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 76/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 77/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 78/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 79/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 80/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 81/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 82/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 83/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "Grid search new best score discovered at iteration 83/500: 0.8236.\n",
            "---------- ----------\n",
            "Grid search run 84/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 85/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 86/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 87/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 88/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 89/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 90/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 91/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 92/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 93/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 94/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 95/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 96/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 97/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "Grid search new best score discovered at iteration 97/500: 0.8243.\n",
            "---------- ----------\n",
            "Grid search run 98/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 99/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 100/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 101/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 102/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 103/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 104/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 105/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 106/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 107/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 108/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 109/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 110/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 111/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 112/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 113/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 114/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 115/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 116/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 117/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 118/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 119/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 120/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 121/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 122/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 123/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 124/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 125/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 126/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 127/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 128/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 129/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 130/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 131/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 132/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 133/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 134/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 135/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 136/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 137/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 138/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 139/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 140/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 141/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 142/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 143/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 144/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 145/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 146/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 147/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 148/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 149/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 150/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 151/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 152/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 153/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 154/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 155/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 156/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 157/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 158/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 159/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 160/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 161/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 162/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 163/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 164/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 165/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 166/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 167/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 168/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 169/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 170/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 171/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 172/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 173/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 174/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 175/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 176/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 177/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.5, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 178/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 179/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 180/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 181/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 182/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 183/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 184/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 185/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 186/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 187/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "Grid search new best score discovered at iteration 187/500: 0.8254.\n",
            "---------- ----------\n",
            "Grid search run 188/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 5}\n",
            "---------- ----------\n",
            "Grid search run 189/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
            "---------- ----------\n",
            "Grid search run 190/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 191/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 192/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
            "---------- ----------\n",
            "Grid search run 193/500:\n",
            "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 12345, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n"
          ]
        }
      ],
      "source": [
        "# start local timer\n",
        "ebm2_tic = time.time()\n",
        "\n",
        "# new grid search that also considers AIR and fairness\n",
        "best_ebm2, ebm_grid_frame = ebm_grid(train, best_ebm_phat, x_names, y_name, gs_params=gs_params, n_models=500,\n",
        "                                     early_stopping_rounds=100, seed=SEED, air_reference='white', air_protected='black',\n",
        "                                     air_cut=0.17)\n",
        "\n",
        "# end local timer\n",
        "ebm2_toc = time.time() - ebm2_tic\n",
        "print('EBM training completed in %.2f s.' % (ebm2_toc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t0aTPYUSJhk"
      },
      "source": [
        "### Display grid search results as table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqA0ujSbXpIg"
      },
      "outputs": [],
      "source": [
        "ebm_grid_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z-QOTTwXqrT"
      },
      "source": [
        "### Display grid search results as plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laQQgXU_XuwF"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "_ = ebm_grid_frame.plot(kind='scatter', x='air', y='auc', title='AIR vs. AUC for EBMs', ax=ax)\n",
        "_ = ax.axvline(x=0.8, color='r', linestyle='--')\n",
        "_ = ax.set_ylim([0.4, 0.85])\n",
        "_ = ax.set_xlim([0.75, 1.05])\n",
        "_ = ax.set_xlabel('AIR')\n",
        "_ = ax.set_ylabel('AUC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVDSeXS2Xx2v"
      },
      "source": [
        "### Retrain most accurate model above 0.8 AIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmTd0AScX0z_"
      },
      "outputs": [],
      "source": [
        "# extract new params dict from ebm_grid_frame\n",
        "rem_params = ebm_grid_frame.loc[ebm_grid_frame['air'] > 0.8].sort_values(by='auc', ascending=False).iloc[0, :].to_dict()\n",
        "\n",
        "# extract features from dict then delete from dict\n",
        "rem_x_names = rem_params['features']\n",
        "del rem_params['features']\n",
        "\n",
        "# record and delete other extraneous information\n",
        "print('Best AUC: %.4f above 0.8 AIR (%.4f).' % (rem_params['auc'], rem_params['air']))\n",
        "del rem_params['auc']\n",
        "del rem_params['air']\n",
        "\n",
        "# reset some parameters to integers\n",
        "rem_params['random_state'] = int(rem_params['random_state'])\n",
        "rem_params['n_jobs'] = int(rem_params['n_jobs'])\n",
        "\n",
        "# retrain\n",
        "rem_ebm = ExplainableBoostingClassifier(**rem_params)\n",
        "rem_ebm.fit(train[rem_x_names], train[y_name])\n",
        "rem_ebm_perf = ROC(rem_ebm.predict_proba).explain_perf(valid[rem_x_names], valid[y_name])\n",
        "rem_auc = rem_ebm_perf._internal_obj['overall']['auc']\n",
        "print('Remediated EBM retrained with AUC: %.4f.' % rem_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4JHJof7X3I0"
      },
      "source": [
        "### Check that other groups are not adversely impacted by change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XJsFXQsZBd1"
      },
      "outputs": [],
      "source": [
        "# create a frame with remediated EBM predictions\n",
        "best_ebm_phat2 = pd.DataFrame(rem_ebm.predict_proba(valid[rem_x_names])[:, 1], columns=['phat'])\n",
        "best_ebm_phat2 = pd.concat([valid.reset_index(drop=True), best_ebm_phat2], axis=1)\n",
        "\n",
        "# calculate new confusion matrices for each group\n",
        "rem_cm_dict2 = {}\n",
        "for name in demographic_group_names:\n",
        "    rem_cm_dict2[name] = get_confusion_matrix(best_ebm_phat2, y_name, 'phat', by=name, level=1, cutoff=0.17, verbose=False)\n",
        "\n",
        "# calculate AIR for each group\n",
        "print('Adverse impact ratio for Asian people vs. White people: %.3f' % air(rem_cm_dict2, 'white', 'asian', verbose=False))\n",
        "print('Adverse impact ratio for Black people vs. White people: %.3f' % air(rem_cm_dict2, 'white', 'black', verbose=False))\n",
        "print('Adverse impact ratio for Females vs. Males: %.3f' % air(rem_cm_dict2, 'male', 'female', verbose=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZo7DjigZEv_"
      },
      "source": [
        "##### This analysis shows that even with a selective cutoff of 0.17, less discriminatory models are available. The new set of features and hyperparameters leads to a ~13% increase in AIR with a ~5% decrease in AUC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNopGJjhZgzP"
      },
      "source": [
        "### Print best model parameters for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJxGPIvIZj7w"
      },
      "outputs": [],
      "source": [
        "rem_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKDgh7HzZkjL"
      },
      "source": [
        "#### Print best model features for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuNPAK6OZof1"
      },
      "outputs": [],
      "source": [
        "rem_x_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dChw67stZ0r7"
      },
      "source": [
        "## New model score files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ5rqcRUaBOn"
      },
      "source": [
        "##### Assign new modelling roles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRk6KDD5Z46N"
      },
      "outputs": [],
      "source": [
        "y_name = 'high_priced'\n",
        "x_names = []\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1qL9IGIaQ1g"
      },
      "source": [
        "### Fit interpretable models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3DDvktyaT8v"
      },
      "source": [
        "##### Split data into train and validation partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwnFrAeoaX2G"
      },
      "outputs": [],
      "source": [
        "np.random.seed(SEED) # preserve exact reproducibility for this cell\n",
        "\n",
        "split_ratio = 0.7 # 70%/30% train/test split\n",
        "\n",
        "# execute split\n",
        "split = np.random.rand(len(data)) < split_ratio\n",
        "train = data[split]\n",
        "valid = data[~split]\n",
        "\n",
        "# summarize split\n",
        "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
        "print('Validation data rows = %d, columns = %d' % (valid.shape[0], valid.shape[1]))\n",
        "\n",
        "# benchmark - Train data rows = 112253, columns = 23\n",
        "# benchmark - Validation data rows = 48085, columns = 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48AkaKosmHBn"
      },
      "source": [
        "### Elastic Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfGKUVm4mJe-"
      },
      "source": [
        "##### Define wrapper function for grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aUyZQm-mN1d"
      },
      "outputs": [],
      "source": [
        "def glm_grid(x_names, y_name, htrain, hvalid, seed):\n",
        "\n",
        "    \"\"\" Wrapper function for penalized GLM with alpha and lambda search.\n",
        "\n",
        "    :param x_names: Names of input features.\n",
        "    :param y_name: Name of target feature.\n",
        "    :param htrain: Training H2OFrame.\n",
        "    :param hvalid: Validation H2OFrame.\n",
        "    :param seed: Random seed for better reproducibility.\n",
        "    :return: Best H2OGeneralizedLinearEstimator.\n",
        "    \"\"\"\n",
        "\n",
        "    alpha_opts = [0.01, 0.25, 0.5, 0.99]  # always keep some L2\n",
        "\n",
        "    # define search criteria\n",
        "    # i.e., over alpha\n",
        "    # lamda search handled by lambda_search param below\n",
        "    hyper_parameters = {'alpha': alpha_opts}\n",
        "\n",
        "    # initialize grid search\n",
        "    grid = H2OGridSearch(\n",
        "        H2OGeneralizedLinearEstimator(family='binomial',\n",
        "                                      lambda_search=True,\n",
        "                                      seed=seed), # seed for grid search\n",
        "        hyper_params=hyper_parameters)\n",
        "\n",
        "\n",
        "    grid.train(y=y_name,\n",
        "               x=x_names,\n",
        "               training_frame=htrain,\n",
        "               validation_frame=hvalid,\n",
        "               seed=seed) # seed for training\n",
        "\n",
        "    # select best model from grid search\n",
        "    best_model = grid.get_grid()[0]\n",
        "    del grid\n",
        "\n",
        "    return best_model\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBE63BVCmRWa"
      },
      "source": [
        "###### Fit elastic net with grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0DWzl1YmWfA"
      },
      "outputs": [],
      "source": [
        "# start local timer\n",
        "glm_tic = time.time()\n",
        "\n",
        "# convert data to h2o frames\n",
        "htrain = h2o.H2OFrame(train)\n",
        "hvalid = h2o.H2OFrame(valid)\n",
        "\n",
        "# train with grid search\n",
        "\n",
        "\n",
        "best_glm = glm_grid(x_names, y_name, htrain, hvalid, SEED)\n",
        "\n",
        "# end local timer\n",
        "glm_toc = time.time() - glm_tic\n",
        "print('Elastic net GLM training completed in %.2f s.' % (glm_toc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atZ1IfiDnJiN"
      },
      "source": [
        "##### Basic AUC assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUFTDGd_nNgZ"
      },
      "outputs": [],
      "source": [
        "print('Validation AUC: %.4f.' % best_glm.auc(valid=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkAWXuuWnP6A"
      },
      "source": [
        "###### Write submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeEGyw_8nUKX"
      },
      "outputs": [],
      "source": [
        "best_glm_submit = best_glm.predict(h2o.H2OFrame(test)).as_data_frame()\n",
        "best_glm_submit.drop(['predict', 'p0'], axis=1, inplace=True)\n",
        "best_glm_submit.columns = ['phat']\n",
        "best_glm_submit.to_csv('group1_best_glm_' + '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Cgy46hnXI8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('group1_best_glm_.csv')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGQ7KBCPoYln"
      },
      "source": [
        "### Monotonic XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1W-DZ82oZDY"
      },
      "source": [
        "##### Define utility function for random grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBGu9kd3olwy"
      },
      "outputs": [],
      "source": [
        "def xgb_grid(dtrain, dvalid, mono_constraints=None, gs_params=None, n_models=None,\n",
        "             ntree=None, early_stopping_rounds=None, verbose=False, seed=None):\n",
        "\n",
        "    \"\"\" Performs a random grid search over n_models and gs_params.\n",
        "\n",
        "    :param dtrain: Training data in LightSVM format.\n",
        "    :param dvalid: Validation data in LightSVM format.\n",
        "    :param mono_constraints: User-supplied monotonicity constraints.\n",
        "    :param gs_params: Dictionary of lists of potential XGBoost parameters over which to search.\n",
        "    :param n_models: Number of random models to evaluate.\n",
        "    :param ntree: Number of trees in XGBoost model.\n",
        "    :param early_stopping_rounds: XGBoost early stopping rounds.\n",
        "    :param verbose: Whether to display training iterations, default False.\n",
        "    :param seed: Random seed for better interpretability.\n",
        "    :return: Best candidate model from random grid search.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # cartesian product of gs_params\n",
        "    keys, values = zip(*gs_params.items())\n",
        "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    # preserve exact reproducibility for this function\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    # select randomly from cartesian product space\n",
        "    selected_experiments = np.random.choice(len(experiments), n_models)\n",
        "\n",
        "    # set global params for objective,  etc.\n",
        "    params = {'booster': 'gbtree',\n",
        "              'eval_metric': 'auc',\n",
        "              'nthread': NTHREAD,\n",
        "              'objective': 'binary:logistic',\n",
        "              'seed': SEED}\n",
        "\n",
        "    # init grid search loop\n",
        "    best_candidate = None\n",
        "    best_score = 0\n",
        "\n",
        "    # grid search loop\n",
        "    for i, exp in enumerate(selected_experiments):\n",
        "\n",
        "        params.update(experiments[exp])  # override global params with current grid run params\n",
        "\n",
        "        print('Grid search run %d/%d:' % (int(i + 1), int(n_models)))\n",
        "        print('Training with parameters:', params)\n",
        "\n",
        "        # train on current params\n",
        "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
        "\n",
        "        if mono_constraints is not None:\n",
        "            params['monotone_constraints'] = mono_constraints\n",
        "\n",
        "        candidate = xgb.train(params,\n",
        "                              dtrain,\n",
        "                              ntree,\n",
        "                              early_stopping_rounds=early_stopping_rounds,\n",
        "                              evals=watchlist,\n",
        "                              verbose_eval=verbose)\n",
        "\n",
        "        # determine if current model is better than previous best\n",
        "        if candidate.best_score > best_score:\n",
        "            best_candidate = candidate\n",
        "            best_score = candidate.best_score\n",
        "            print('Grid search new best score discovered at iteration %d/%d: %.4f.' %\n",
        "                             (int(i + 1), int(n_models), candidate.best_score))\n",
        "\n",
        "        print('---------- ----------')\n",
        "\n",
        "    return best_candidate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "214s8sfVozoD"
      },
      "source": [
        "##### Fit monotonic XGBoost with random grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR52KtlJo4rH"
      },
      "outputs": [],
      "source": [
        "# dictionary of hyperparameter value lists for grid search\n",
        "gs_params = {'colsample_bytree': [0.3, 0.5, 0.7, 0.9],\n",
        "             'colsample_bylevel': [0.3, 0.5, 0.7, 0.9],\n",
        "             'eta': [0.005, 0.05, 0.5],\n",
        "             'max_depth': [3, 5, 7],\n",
        "             'reg_alpha': [0.0005, 0.005, 0.05],\n",
        "             'reg_lambda': [0.0005, 0.005, 0.05],\n",
        "             'subsample': [0.3, 0.5, 0.7, 0.9],\n",
        "             'min_child_weight': [1, 5, 10],\n",
        "             'gamma': [0.0, 0.1, 0.2 , 0.3, 0.4]}\n",
        "\n",
        "# define monotonicity constraints\n",
        "mono_constraints = tuple([int(i) for i in np.sign(train[x_names + [y_name]].corr()[y_name].values[:-1])])\n",
        "\n",
        "# start local timer\n",
        "mxgb_tic = time.time()\n",
        "\n",
        "# Convert data to SVMLight format\n",
        "dtrain = xgb.DMatrix(train[x_names], train[y_name])\n",
        "dvalid = xgb.DMatrix(valid[x_names], valid[y_name])\n",
        "\n",
        "# Monotonic XGBoost grid search\n",
        "best_mxgb = xgb_grid(dtrain, dvalid, gs_params=gs_params, n_models=50, ntree=1000, early_stopping_rounds=100,\n",
        "                     mono_constraints=mono_constraints, seed=SEED)\n",
        "\n",
        "# end local timer\n",
        "mxgb_toc = time.time() - mxgb_tic\n",
        "print('Monotonic GBM training completed in %.2f s.' % (mxgb_toc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJ_i0J8pBsO"
      },
      "source": [
        "##### Basic AUC assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij0SJTvhpE0F"
      },
      "outputs": [],
      "source": [
        "print('Validation AUC: %.4f.' % best_mxgb.best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yUlYSTqpG6y"
      },
      "source": [
        "##### Write submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBe9kXAjpLaS"
      },
      "outputs": [],
      "source": [
        "dtest = xgb.DMatrix(test[x_names])\n",
        "best_mxgb_submit = pd.DataFrame(best_mxgb.predict(dtest, iteration_range=(0, best_mxgb.best_ntree_limit)), columns=['phat'])\n",
        "best_mxgb_submit.to_csv('group1_best_mxgb_.csv',index=False)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9DBDUjVpNmr"
      },
      "outputs": [],
      "source": [
        "\n",
        "files.download('group1_best_mxgb_.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzFWQJYfpPsZ"
      },
      "source": [
        "### Explainable Boosting Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKTNVQJqWQc"
      },
      "source": [
        "##### Define utility function for random grid search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lokfx4a3qZ11"
      },
      "outputs": [],
      "source": [
        "def ebm_grid(train, valid, x_names, y_name, gs_params=None, n_models=None, early_stopping_rounds=None, seed=None):\n",
        "\n",
        "    \"\"\" Performs a random grid search over n_models and gs_params.\n",
        "\n",
        "    :param train: Training data as Pandas DataFrame.\n",
        "    :param valid: Validation data as Pandas DataFrame.\n",
        "    :param x_names: Names of input features.\n",
        "    :param y_name: Name of target feature.\n",
        "    :param gs_params: Dictionary of lists of potential EBM parameters over which to search.\n",
        "    :param n_models: Number of random models to evaluate.\n",
        "    :param early_stopping_rounds: EBM early stopping rounds.\n",
        "    :param seed: Random seed for better interpretability.\n",
        "    :return: Best candidate model from random grid search.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # cartesian product of gs_params\n",
        "    keys, values = zip(*gs_params.items())\n",
        "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    # preserve exact reproducibility for this function\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    # select randomly from cartesian product space\n",
        "    selected_experiments = np.random.choice(len(experiments), n_models)\n",
        "\n",
        "    # set global params for seed, etc.\n",
        "    params = {'n_jobs': NTHREAD,\n",
        "              'early_stopping_rounds': early_stopping_rounds,\n",
        "              'random_state': SEED}\n",
        "\n",
        "    # init grid search loop\n",
        "    best_candidate = None\n",
        "    best_score = 0\n",
        "\n",
        "    # grid search loop\n",
        "    for i, exp in enumerate(selected_experiments):\n",
        "\n",
        "        params.update(experiments[exp])  # override global params with current grid run params\n",
        "\n",
        "        print('Grid search run %d/%d:' % (int(i + 1), int(n_models)))\n",
        "        print('Training with parameters:', params)\n",
        "\n",
        "        # train\n",
        "        ebm = ExplainableBoostingClassifier(**params)\n",
        "        candidate = ebm.fit(train[x_names], train[y_name])\n",
        "\n",
        "        # calculate AUC\n",
        "        ebm_perf = ROC(ebm.predict_proba).explain_perf(valid[x_names], valid[y_name])\n",
        "        candidate_best_score = ebm_perf._internal_obj['overall']['auc']\n",
        "\n",
        "        # determine if current model is better than previous best\n",
        "        if candidate_best_score > best_score:\n",
        "            best_candidate = candidate\n",
        "            best_score = candidate_best_score\n",
        "            print('Grid search new best score discovered at iteration %d/%d: %.4f.' %\n",
        "                             (int(i + 1), int(n_models), candidate_best_score))\n",
        "\n",
        "        print('---------- ----------')\n",
        "\n",
        "        del ebm\n",
        "\n",
        "    return best_candidate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKw5CGn2qdcO"
      },
      "source": [
        "##### Fit EBM with random grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Y2kkr5qndS"
      },
      "outputs": [],
      "source": [
        "# dictionary of hyperparameter value lists for grid search\n",
        "gs_params = {'max_bins': [128, 256, 512],\n",
        "             'max_interaction_bins': [16, 32, 64],\n",
        "             'interactions': [5, 10, 15],\n",
        "             'outer_bags': [4, 8, 12],\n",
        "             'inner_bags': [0, 4],\n",
        "             'learning_rate': [0.001, 0.01, 0.05],\n",
        "             'validation_size': [0.1, 0.25, 0.5],\n",
        "             'min_samples_leaf': [1, 2, 5, 10],\n",
        "             'max_leaves': [1, 3, 5]}\n",
        "\n",
        "# start local timer\n",
        "ebm_tic = time.time()\n",
        "\n",
        "# EBM grid search\n",
        "best_ebm = ebm_grid(train, valid, x_names, y_name, gs_params=gs_params, n_models=10,\n",
        "                    early_stopping_rounds=100, seed=SEED)\n",
        "\n",
        "# end local timer\n",
        "ebm_toc = time.time() - ebm_tic\n",
        "print('EBM training completed in %.2f s.' % (ebm_toc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD1tlmOqqrl2"
      },
      "source": [
        "##### Basic AUC assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJFrpfrGqvms"
      },
      "outputs": [],
      "source": [
        "best_ebm_perf = ROC(best_ebm.predict_proba).explain_perf(valid[x_names], valid[y_name])\n",
        "print('Validation AUC: %.4f.' % best_ebm_perf._internal_obj['overall']['auc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5b9p6hrqypJ"
      },
      "source": [
        "##### Write submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPU2UF8Fq1vd"
      },
      "outputs": [],
      "source": [
        "best_ebm_submit = pd.DataFrame(best_ebm.predict_proba(test[x_names])[:, 1], columns=['phat'])\n",
        "best_ebm_submit.to_csv('group1_best_ebm_.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGDiGBnSq4yS"
      },
      "outputs": [],
      "source": [
        "files.download('group1_best_ebm_.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tTOwYBhZqme"
      },
      "source": [
        "##### End timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY1HaCPjZtd2"
      },
      "outputs": [],
      "source": [
        "toc = time.time() - tic\n",
        "print('All tasks completed in %.2f s.' % (toc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}